{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as mt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1. Observations\n",
    "\n",
    "<br>\n",
    "The problem is the following : \n",
    "\\begin{equation} min f(x) \\end{equation}\n",
    "\\begin{equation} x ∈ {\\Omega} \\end{equation}\n",
    "\\begin{equation} f(x_0, x_1, x_2) = x_0^3 + x_1^3 + x_2^3 \\end{equation}\n",
    "\\begin{equation} Ω = \\{x ∈ R^3 : x_0^2 + x_1^2 + x_2^2 = 1\\} \\end{equation}\n",
    " \n",
    " <br> The restriction of Ω implies that x0, x1 and x2 belong to [-1, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#\n",
    "#def h(x1, x2):\n",
    "#    return mt.sqrt(1 - mt.pow(x1,2) - mt.pow(x2,2))\n",
    "#\n",
    "#def h_neg(x1, x2):\n",
    "#    return -mt.sqrt(1 - mt.pow(x1,2) - mt.pow(x2,2))\n",
    "#\n",
    "#\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111, projection='3d')\n",
    "#x = y = np.arange(-0.7, 0.7, 0.05)\n",
    "#X, Y = np.meshgrid(x, y)\n",
    "#z1s = np.array([h(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))])\n",
    "#Z1 = z1s.reshape(X.shape)\n",
    "#\n",
    "#ax.plot_surface(X, Y, Z1)\n",
    "#\n",
    "#z2s = np.array([h_neg(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))])\n",
    "#Z2 = z2s.reshape(X.shape)\n",
    "#\n",
    "#ax.plot_surface(X, Y, Z2)\n",
    "#\n",
    "#\n",
    "#ax.set_xlabel('X0')\n",
    "#ax.set_ylabel('X1')\n",
    "#ax.set_zlabel('X2')\n",
    "#plt.title('domain valid Ω')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plot](img/plot.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Stationary points\n",
    "To find the stationary points, we resolve the system $grad_f = 0$. <br>\n",
    "The only stationary point of f(x) is $x^* = (0, 0, 0)$, which is invalid (out of our domain ${\\Omega}$), as we can see on the plot above\n",
    "\n",
    "#### b. Convexity\n",
    "There are several ways to examine the convexity of a function. We will check the Hessian matrix. If it is semi-definite positive, then the function is convexe.\n",
    "Here the Hessian is defined by <br>\n",
    "\n",
    "$$\\begin{vmatrix}\n",
    "6x_0&0&0\\\\ \n",
    "0&6x_1&0\\\\ \n",
    "0&0&6x_2 \n",
    "\\end{vmatrix}$$\n",
    " \n",
    "The 3 eigen values are $6x_0$, $6x_1$ and $6x_2$. Regarding the domain, those eigen values can be either positive or negative. Hence for the points x ∈ ${\\Omega}$, the Hessian is not definite positive, neither definite negative. Also, the function f(x) is not convexe neither concave for all x ∈ ${\\Omega}$.\n",
    "\n",
    "### 2.  Restricted problem\n",
    "\n",
    "#### a. KKT\n",
    "Before resolving a KKT system we need to make sure that a condition is verified.<br>\n",
    "We will use the LICQ conditions which implies that union of the gradients of all the restrictions (equations and inequations) are linearly independant vectors.<br>\n",
    "In our case, we only have one restriction equation : the only gradient is by definition linearly independant with himself. The LICQ conditions is verified.<br>\n",
    "The KKT method gives the following system : <br>\n",
    "$$-3x_0^2 = 2\\lambda_1x_0$$\n",
    "$$-3x_1^2 = 2\\lambda_1x_1$$\n",
    "$$-3x_2^2 = 2\\lambda_1x_2$$\n",
    " \n",
    "which has one solution $x^* = (-2\\lambda_1/3, -2\\lambda_1/3, -2\\lambda_1/3)$\n",
    "\n",
    "### 3. Irrestricted problem\n",
    "In order to change our restricted problem into an irrestricted one. We have to use the method of intern or extern penality\n",
    "\n",
    "#### a. Extern penality\n",
    "We will use the penality function : <br>\n",
    "P(x) = $\\Sigma$ ${h_i^2}$(x) + $\\Sigma$[${max(0, g_i(x))^2}$] <br>\n",
    "  \n",
    "In our case, our objective function becomes:\n",
    "\n",
    "\\begin{equation} f(x_0, x_1, x_2) = x_0^3 + x_1^3 + x_2^3 + \\rho{(x_0^2 + x_1^2 + x_2^2 - 1)^2} \\end{equation}\n",
    "\n",
    "With $\\rho > 0$,\n",
    "\n",
    "The extern penality has the advantages to be well defined in the valid and invalid space. However, the value of $\\rho$ needs to tend to infinity in order to approach the valid solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x, ro):\n",
    "    return mt.pow(x[0], 3) + mt.pow(x[1], 3) + mt.pow(x[2], 3) + ro * (mt.pow((mt.pow(x[0], 2) + mt.pow(x[1], 2) + mt.pow(x[2], 2) - 1),2))\n",
    "\n",
    "def grad_f(x, ro):\n",
    "    return np.array([3*mt.pow(x[0],2) + 4*ro*x[0]*((mt.pow(x[0], 2) + mt.pow(x[1], 2) + mt.pow(x[2], 2) - 1)),\n",
    "                     3*mt.pow(x[1],2) + 4*ro*x[1]*((mt.pow(x[0], 2) + mt.pow(x[1], 2) + mt.pow(x[2], 2) - 1)),\n",
    "                     3*mt.pow(x[2],2) + 4*ro*x[2]*((mt.pow(x[0], 2) + mt.pow(x[1], 2) + mt.pow(x[2], 2) - 1))])\n",
    "\n",
    "def hessian_f(x, ro):\n",
    "    return np.array([\n",
    "        [6*x[0] + mt.pow(12*ro*x[0], 2) + 4*ro*(mt.pow(x[1],2)) + 4*ro*(mt.pow(x[2],2)) - 4*ro , 8*ro*x[0]*x[1], 8*ro*x[0]*x[2]],\n",
    "        [8*ro*x[1]*x[0], 6*x[1] + mt.pow(12*ro*x[1], 2) + 4*ro*(mt.pow(x[0],2)) + 4*ro*(mt.pow(x[2],2)) - 4*ro, 8*ro*x[1]*x[2]],\n",
    "        [8*ro*x[2]*x[0], 8*ro*x[2]*x[1], 6*x[2] + mt.pow(12*ro*x[2], 2) + 4*ro*(mt.pow(x[0],2)) + 4*ro*(mt.pow(x[1],2)) - 4*ro]\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Intern penality\n",
    "In our case, the problem doesn't include restrictive inequations. Hence, we can't use the Intern penality method.\n",
    "\n",
    "Now that we have a new objective function, we can look for its minimum using research algorithm.\n",
    "In our case we will use the Gradient and Newton algorithm.\n",
    " \n",
    "#### c. Gradient Descent\n",
    "\n",
    "The Gradient Descent algorithm try to find a converging sequence of $x^k$ until it reaches a local minimum ($grad_f = 0$) of the function we tried to minimize. It uses the - gradient as a decreasing direction and the step will be found by the rules of Armijo. <br>\n",
    "In order to avoid overflow, we use an iteration limitation k_max = 200.\n",
    " \n",
    "The Armijo rule finds a t (step) that verifies for $x_k$ ∈ Ω, n ∈ [0,1]  :\n",
    "    \\begin{equation} f(x_k + td) \\leq f(x_k) + n \\cdot t \\cdot grad_f(x_k)^T \\cdot d \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 200\n",
    "x_0 = [0, 0, -1]\n",
    "epsilon = mt.pow(10, -1)\n",
    "ro = 50\n",
    "\n",
    "\n",
    "def armijo(x_k, d_k):\n",
    "    # initialisation\n",
    "    gamma = 0.8\n",
    "    n = 1/4\n",
    "    t = 0.3\n",
    "    k = 0\n",
    "    while ( ((f(x_k + t*d_k, ro)) > (f(x_k, ro) + n*t*np.dot(grad_f(x_k, ro), d_k))) & (k < max_k) ):\n",
    "        t = t*gamma\n",
    "        k += 1\n",
    "    \n",
    "    return t, k\n",
    "\n",
    "def gradient_descent(x_0, ro):\n",
    "    # intiatilisation\n",
    "    x_k = x_0\n",
    "    k = 0\n",
    "    k_armijo = 0\n",
    "    # convergence\n",
    "    while ( (np.linalg.norm(grad_f(x_k, ro)) > epsilon) & (k < max_k) ):\n",
    "        d_k = - grad_f(x_k, ro)\n",
    "        t, k_arm = armijo(x_k, d_k)\n",
    "        x_k = x_k + t * d_k\n",
    "        k += 1\n",
    "        k_armijo += k_arm\n",
    "    \n",
    "    # return the local minimum, the number of iterations, and the number of armijo iterations\n",
    "    return x_k, k, k_armijo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent method results:\n",
      "\n",
      "Parameters: \n",
      "x_initial = (0, 0, -1), max iterations: 200, ro: 50\n",
      "\n",
      "At k = 0, f(x_k) = -1.0\n",
      "At k = 4, x_k = (0.0, 0.0, -1.00735200189), f(x_k) = -1.011328548435949, total Armijo calls: 80\n"
     ]
    }
   ],
   "source": [
    "# proof of work\n",
    "print(\"Gradient Descent method results:\\n\")\n",
    "print(\"Parameters: \\n\" + \"x_initial = (\" + str(x_0[0]) + \", \" + str(x_0[1]) + \", \" + str(x_0[2]) + \"), max iterations: \" + str(max_k) + \", ro: \" + str(ro) + \"\\n\")\n",
    "print(\"At k = 0, f(x_k) = \" + str(f(x_0,ro)))\n",
    "x_star, k_star, k_armijo = gradient_descent(x_0, ro)\n",
    "print(\"At k = \" + str(k_star) + \", x_k = (\" + str(x_star[0]) + \", \" + str(x_star[1]) + \", \" + str(x_star[2]) + \"), f(x_k) = \" + str(f(x_star, ro)) + \", total Armijo calls: \" + str(k_armijo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Quase Newton\n",
    "In this method we aim to use qualities of both methods: Newton's accuracy and gradient method's low computational cost.<br>\n",
    "Hence, we use a different decision $d_k$. In the quase Newton method, the direction is calculated by multiplicating the Hessian $H_k$ with the gradient at a step k.\n",
    "\n",
    "\\begin{equation} d_k = - H_k \\cdot grad_f(k)  \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def quase_newton(x_0, ro):\n",
    "    x_k = x_0\n",
    "    k = 0\n",
    "    t = 0\n",
    "    k_armijo = 0\n",
    "    while ( (np.linalg.norm(grad_f(x_k, ro)) > epsilon) & (k < max_k) ):\n",
    "        d_k = - np.dot(hessian_f(x_k, ro), grad_f(x_k, ro))\n",
    "        t, k_arm = armijo(x_k, d_k)\n",
    "        x_k = x_k + t*d_k\n",
    "        k += 1\n",
    "        k_armijo += k_arm\n",
    "        \n",
    "    return x_k, k, k_armijo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quase Newton method results:\n",
      "\n",
      "Parameters: \n",
      "x_initial = (0, 0, -1), max iterations: 200, ro: 50\n",
      "\n",
      "At k = 0, f(x_k) = -1.0\n",
      "At k = 3, x_k = (0.0, 0.0, -1.00771248575), f(x_k) = -1.0113279476309514, total Armijo calls: 233\n"
     ]
    }
   ],
   "source": [
    "# proof of work\n",
    "print(\"Quase Newton method results:\\n\")\n",
    "print(\"Parameters: \\n\" + \"x_initial = (\" + str(x_0[0]) + \", \" + str(x_0[1]) + \", \" + str(x_0[2]) + \"), max iterations: \" + str(max_k) + \", ro: \" + str(ro) + \"\\n\")\n",
    "print(\"At k = 0, f(x_k) = \" + str(f(x_0,ro)))\n",
    "x_star, k_star, k_armijo = quase_newton(x_0, ro)\n",
    "print(\"At k = \" + str(k_star) + \", x_k = (\" + str(x_star[0]) + \", \" + str(x_star[1]) + \", \" + str(x_star[2]) + \"), f(x_k) = \" + str(f(x_star, ro)) + \", total Armijo calls: \" + str(k_armijo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Results\n",
    "#### a. Gradient Descent\n",
    "| $x^0$ | Iterations | Calls Armijo | Opt. Point | Opt. Value | Error\n",
    "|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:\n",
    "|Valor|Valor|Valor|Valor|Valor|Valor\n",
    "|Valor|Valor|Valor|Valor|Valor|Valor\n",
    "|Valor|Valor|Valor|Valor|Valor|Valor\n",
    "|Valor|Valor|Valor|Valor|Valor|Valor\n",
    "#### b. Quase Newton\n",
    "| $x^0$ | Iterations | Calls Armijo | Opt. Point | Opt. Value | Error\n",
    "|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:\n",
    "|Valor|Valor|Valor|Valor|Valor|Valor\n",
    "|Valor|Valor|Valor|Valor|Valor|Valor\n",
    "|Valor|Valor|Valor|Valor|Valor|Valor\n",
    "|Valor|Valor|Valor|Valor|Valor|Valor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
